{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.summarization.bm25 import get_bm25_weights\n",
    "from rank_bm25 import BM25Okapi\n",
    "import json\n",
    "from termcolor import colored\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the json file from kevin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../output_data/tmp/article_pool.json', 'r') as input_file:\n",
    "    phrase_list = json.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_layer_1 = pd.DataFrame(phrase_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the layer number\n",
    "layer_num = len(phrase_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the pair of unique phrase in layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_phrase_list = np.unique(data_df_layer_1['phrase'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['3 1.9', '3 Ca', '3 aromatic ring', '3 c', '3 reduction', 'Ca 1.9',\n",
       "       'Ca aromatic ring', 'Ca c', 'Ca reduction', 'MccA 1.9', 'MccA 3',\n",
       "       'MccA Ca', 'MccA PSII', 'MccA Sr', 'MccA aromatic ring', 'MccA c',\n",
       "       'MccA reduction', 'MccA sulfite', 'PSII 1.9', 'PSII 3', 'PSII Ca',\n",
       "       'PSII aromatic ring', 'PSII c', 'PSII reduction', 'PSII sulfite',\n",
       "       'Sr 1.9', 'Sr 3', 'Sr Ca', 'Sr PSII', 'Sr aromatic ring', 'Sr c',\n",
       "       'Sr reduction', 'Sr sulfite', 'aromatic ring 1.9',\n",
       "       'aromatic ring c', 'aromatic ring reduction', 'c 1.9',\n",
       "       'c reduction', 'reduction 1.9', 'sulfite 1.9', 'sulfite 3',\n",
       "       'sulfite Ca', 'sulfite aromatic ring', 'sulfite c',\n",
       "       'sulfite reduction'], dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_phrase_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the unique phrase in layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list = list(pd.read_json('../output_data/tmp/selected_phrases.json',typ='series')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MccA',\n",
       " 'Sr',\n",
       " 'PSII',\n",
       " 'sulfite',\n",
       " '3',\n",
       " 'Ca',\n",
       " 'aromatic ring',\n",
       " 'c',\n",
       " 'reduction',\n",
       " '1.9']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create list of list(unique phrase) for current pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop throught every pair of phrase\n",
    "list_of_phrase_list = []\n",
    "for i in range(len(unique_phrase_list)):\n",
    "    tmplist =[]\n",
    "    for j in range(len(p_list)):\n",
    "        if p_list[j] in unique_phrase_list[i]:\n",
    "            tmplist.append(p_list[j])\n",
    "    list_of_phrase_list.append(tmplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentece_list(unique_phrase_list):\n",
    "    list_of_phrase_list = []\n",
    "    for i in range(len(unique_phrase_list)):\n",
    "        tmplist =[]\n",
    "        for j in range(len(p_list)):\n",
    "            if p_list[j] in unique_phrase_list[i]:\n",
    "                tmplist.append(p_list[j])\n",
    "        list_of_phrase_list.append(tmplist)\n",
    "    return list_of_phrase_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attention: Every sentence in the articles from the pool is a candidate sentence for the final summary.\n",
    "# step: compute a BM25 score for every candidate sentence\n",
    "\n",
    "# to compute BM25\n",
    "# 1. find all sentence in current layer(done)\n",
    "# 2. index all sentence(done)\n",
    "# 3. compute score for all current sentence\n",
    "# 4. question here-> should we use the sentence with highest score \n",
    "# to cover the phrase? or to cover pair of phrase?\n",
    "# If so, after phrase been recoverd, tag the sentence been used,\n",
    "# recompute the BM25 in unused sentence pool\n",
    "# and iterate all sentece until all phrase been coverd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the article pool now\n",
    "article_list = data_df_layer_1['article'].values\n",
    "#for each article, find all sentence\n",
    "article_list[0]\n",
    "sentence_dic = {}\n",
    "list_sentence = []\n",
    "s_count = 0 #sentence index\n",
    "for i in range(len(article_list)):\n",
    "    #for every sentence, if not in sentence_list, push sentence in list\n",
    "    tmp_sentence_list = article_list[i].split(\". \")    \n",
    "    for j in range(len(tmp_sentence_list)):\n",
    "#         if tmp_sentence_list[j] not in article_list:\n",
    "        sentence_dic[s_count] = tmp_sentence_list[j]\n",
    "        list_sentence.append(tmp_sentence_list[j])        \n",
    "        s_count +=1\n",
    "list_sentence = np.unique(list_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_pool(data_df_layer_1):\n",
    "    article_list = data_df_layer_1['article'].values\n",
    "    #for each article, find all sentence\n",
    "    article_list[0]\n",
    "    sentence_dic = {}\n",
    "    list_sentence = []\n",
    "    s_count = 0 #sentence index\n",
    "    for i in range(len(article_list)):\n",
    "        #for every sentence, if not in sentence_list, push sentence in list\n",
    "#         tmp_sentence_list = article_list[i].split(\".\")\n",
    "        tmp_sentence_list = re.split('(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', article_list[i])\n",
    "        for j in range(len(tmp_sentence_list)):\n",
    "#          if tmp_sentence_list[j] not in article_list:\n",
    "            sentence_dic[s_count] = tmp_sentence_list[j]\n",
    "            list_sentence.append(tmp_sentence_list[j])\n",
    "            s_count +=1\n",
    "    return np.unique(list_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25_score(sentence_list,unique_phrase_list,p_list):\n",
    "    #at each iteration\n",
    "    #find the sentence that could lead to the highest bm25\n",
    "        #1.for every pair of phrase, find the max score of that sentence, save that score and the respective sentence,\n",
    "        #2.pick the highest score, also recover that sentence\n",
    "        #3.find how many phrase this sentece been touched, tag those phrase, pop those phrase out of phrase list\n",
    "        #4.if pair of phrase that lead to highest score contains all some at least two of phrase that been poped out,\n",
    "        #5.pop out that phrase pair in pair list\n",
    "        #6.finsihed current iteration\n",
    "        #7.check whether the lenght of phrass list becomes 0 or num of iteration hit max\n",
    "        #8.if either happens, exit the problem, return the sentence list, and the touched phrase list\n",
    "        \n",
    "    \n",
    "    answer_sentence_list = []\n",
    "    touched_phrase_list = []\n",
    "    count = 0\n",
    "    while len(p_list) > 0 and len(touched_phrase_list) < len(p_list):\n",
    "        #step1\n",
    "        #create a data structure to save score for current query(as pair of phrase)\n",
    "        #in the future, implement the max-heap ds here O(nlogn) push, O(1) peek\n",
    "        score_list_current_iter = []\n",
    "        sentence_idx_list = []\n",
    "        cur_pair_phrase_dic = {}\n",
    "        for i in range(len(unique_phrase_list)):\n",
    "            #compute bm25 score use current phrase pair as query\n",
    "            bm25 = BM25Okapi(sentence_list) #create class of bm25\n",
    "            doc_scores = bm25.get_scores(unique_phrase_list[i])\n",
    "            sentence_loc = np.argmax(doc_scores)  #the index num of max-score sentence in the sen_list\n",
    "            sentence_score = np.max(doc_scores)   #the max score over all score of sentence for current query\n",
    "            score_list_current_iter.append(sentence_score)\n",
    "            sentence_idx_list.append(sentence_loc)\n",
    "            if sentence_score not in cur_pair_phrase_dic:\n",
    "                cur_pair_phrase_dic[sentence_score] = i\n",
    "        #step2\n",
    "        highest_score_index = np.argmax(score_list_current_iter)\n",
    "        high_sen = sentence_list[sentence_idx_list[highest_score_index]] \n",
    "        answer_sentence_list.append(high_sen)\n",
    "        #step three\n",
    "        cur_touched_phrase = []  #record how many phrase been touched by this sentence\n",
    "#         print('current lengh of p_list is', len(p_list))\n",
    "        list_deleted = []\n",
    "        for pos in range(len(p_list)):\n",
    "#             print('current pos is', pos)\n",
    "#             print('what is that ', p_list[pos])\n",
    "            if p_list[pos] in high_sen:\n",
    "                if p_list[pos] not in cur_touched_phrase:\n",
    "                    cur_touched_phrase.append(p_list[pos])\n",
    "#                 p_list.remove(p_list[pos])\n",
    "                #remove phrase list\n",
    "        #<wait>\n",
    "#         for i in range(len(cur_touched_phrase)):\n",
    "#             if cur_touched_phrase[i] in p_list:\n",
    "#                 p_list.remove(cur_touched_phrase[i])\n",
    "        #<wait>\n",
    "#         print('current sentence is', sentence_list[highest_score_index])\n",
    "#         print('len of unique pair  list', len(unique_phrase_list))\n",
    "        print('lengh of touched phrase', len(cur_touched_phrase))\n",
    "#         print('current count is', count)\n",
    "#         print('lengh of sentence list', len(sentence_list))\n",
    "\n",
    "#         #if the current pair of phrase that lead to this sentence which has max score contains two phrase in cur_touched_phrase\n",
    "#         #pop this pair of phrase out of pair of phrase list\n",
    "#         #return the pair of phrase by useing dic\n",
    "        curpair = unique_phrase_list[cur_pair_phrase_dic[np.max(score_list_current_iter)]] \n",
    "#         #count how many phrase in cur_touched_phrase \n",
    "        count_now = 0\n",
    "        for loc in range(len(cur_touched_phrase)):\n",
    "            if cur_touched_phrase[loc] in curpair:\n",
    "                count_now+=1\n",
    "        if count_now>=2 and curpair in unique_phrase_list:\n",
    "#             print('hahahahahahhaa six six six')\n",
    "            unique_phrase_list.remove(curpair)\n",
    "        count +=1\n",
    "        #remove phrase list\n",
    "        #unique_phrase_list.remove(curpair)\n",
    "        #remove sentence\n",
    "        sentence_list.remove(high_sen)\n",
    "        \n",
    "        #add cur touched list to total touched list\n",
    "        for inx in range(len(cur_touched_phrase)):\n",
    "            if cur_touched_phrase[inx] not in touched_phrase_list:\n",
    "                touched_phrase_list.append(cur_touched_phrase[inx])\n",
    "        print('now total touched length', len(touched_phrase_list))\n",
    "        #create a pair phrase function based on the cur_touched_phrase_process\n",
    "        tmp_phrase_pair = []\n",
    "        for p in range(len(cur_touched_phrase)):\n",
    "            for q in range(p,len(cur_touched_phrase)):\n",
    "                if cur_touched_phrase[p] != cur_touched_phrase[q]:\n",
    "                    tmp_phrase_pair.append([cur_touched_phrase[p],cur_touched_phrase[q]])\n",
    "        #iter thorught unique_phrase_list if pair is exist in touched list, dequeue them\n",
    "        \n",
    "        deletelist = []\n",
    "#         print('tmo coutched list is', len(tmp_phrase_pair))\n",
    "#         print('tmp phrase pari look like ', tmp_phrase_pair)\n",
    "        for p2 in range(len(tmp_phrase_pair)):\n",
    "            for q2 in range(len(unique_phrase_list)):\n",
    "                if tmp_phrase_pair[p2][0] in unique_phrase_list[q2] and tmp_phrase_pair[p2][1] in unique_phrase_list[q2]:\n",
    "                    if unique_phrase_list[q2] not in deletelist:\n",
    "                        deletelist.append(unique_phrase_list[q2])\n",
    "        \n",
    "#         print('look of deletelist, ', deletelist)\n",
    "#         print('len of delete list', len(deletelist))\n",
    "        for p3 in range(len(deletelist)):\n",
    "            if deletelist[p3] in unique_phrase_list:\n",
    "                unique_phrase_list.remove(deletelist[p3])\n",
    "#         print('len of unique list is', len(unique_phrase_list))\n",
    "        print('num of iteration now is', count)\n",
    "\n",
    "    return answer_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cover(sentence_list,unique_phrase_list,p_list):\n",
    "    #at each iteration\n",
    "        #find the sentence that cover most number of unvisted phrase\n",
    "        #mark those phrase as visited (pop)\n",
    "        #mark the sentence as visited (pop)\n",
    "    answer_sentence_list = []\n",
    "    touch_count_dic = []\n",
    "    count = 0\n",
    "    while len(p_list) > 0:\n",
    "        #create a data structure to save how many unvisited phrase the current sentence touched\n",
    "        touch_count_dic = {} #key as num of phrase touched, value is a list of index of sentence\n",
    "        global_max_count = 0\n",
    "        for i in range(len(sentence_list)):\n",
    "            #compute num of touched\n",
    "            tmpcount = 0\n",
    "            for j in range(len(p_list)):\n",
    "                if p_list[j] in sentence_list[i]:\n",
    "                    tmpcount+=1\n",
    "            if tmpcount > global_max_count:\n",
    "                global_max_count = tmpcount\n",
    "            # save current result in dic\n",
    "            if tmpcount in touch_count_dic:\n",
    "                #return the list\n",
    "                curlist = touch_count_dic.get(tmpcount)\n",
    "                curlist.append(i)\n",
    "            else:\n",
    "                tmplist = []\n",
    "                tmplist.append(i)\n",
    "                touch_count_dic[tmpcount] = tmplist\n",
    "        #use the global max count to return lit of index of sentence that lead to the max current count\n",
    "        \n",
    "        list_of_max_index_sentence = touch_count_dic[global_max_count]\n",
    "        #pick the first one\n",
    "        selected_max_sentence_index = list_of_max_index_sentence[0]\n",
    "        selected_max_sentence = sentence_list[selected_max_sentence_index]\n",
    "        #set cover\n",
    "        visited_list = []\n",
    "#         print('what is sentence now', selected_max_sentence)\n",
    "        \n",
    "        for loc in range(len(p_list)):\n",
    "            if p_list[loc] in selected_max_sentence and p_list[loc] not in visited_list:\n",
    "                visited_list.append(p_list[loc])\n",
    "        #delete all visited list\n",
    "        print('!!!! visted list is', len(visited_list))\n",
    "#         print('what is sentence now', selected_max_sentence)\n",
    "        for pos2 in range(len(visited_list)):\n",
    "            p_list.remove(visited_list[pos2])\n",
    "        answer_sentence_list.append(selected_max_sentence)\n",
    "        #remove the current sentencn\n",
    "        sentence_list.pop(selected_max_sentence_index)\n",
    "#         print('length of sentence list is', len(sentence_list))\n",
    "        print('len of remainng list', p_list)\n",
    "    return answer_sentence_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotating_function(answer,p_list): #only mark the first occurance of a phrase exist in sentence\n",
    "    #iter through every answer\n",
    "    for i in range(len(answer)):\n",
    "        for j in range(len(p_list)):\n",
    "            if p_list[j] in answer[i]:\n",
    "                #find the starting index\n",
    "                start = answer[i].find(p_list[j])\n",
    "                end = start + len(p_list[j])\n",
    "                answer[i] = answer[i][0:start] + colored(p_list[j],'red') + answer[i][end:]\n",
    "    for i in range(len(answer)):\n",
    "        print('index :', i, '', answer[i] +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_func():\n",
    "    with open('../output_data/tmp/article_pool.json', 'r') as input_file:\n",
    "        phrase_list = json.load(input_file)\n",
    "    layer_num = len(phrase_list)  #how many layer\n",
    "    for i in range(layer_num):\n",
    "        data_df_layer_1 = pd.DataFrame(phrase_list[i])\n",
    "        unique_phrase_list = np.unique(data_df_layer_1['phrase'].values)\n",
    "        p_list = list(pd.read_json('../output_data/tmp/selected_phrases.json',typ='series')[i])\n",
    "#         list_of_phrase_list =  create_sentece_list(unique_phrase_list)\n",
    "        list_sentence = create_sentence_pool(data_df_layer_1)\n",
    "        #run\n",
    "        answer = set_cover(list(list_sentence),list(unique_phrase_list),p_list.copy())\n",
    "        #save it to json file\n",
    "        path = \"../output_data/out_2d_layer_\" + str(i)\n",
    "\n",
    "        with open(path, 'w') as outfile:\n",
    "            json.dump(answer, outfile)\n",
    "            \n",
    "        print('current layer is: ',i )\n",
    "        annotating_function(answer.copy(),p_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!! visted list is 5\n",
      "len of remainng list ['MccA', 'sulfite', '3', 'aromatic ring', 'reduction']\n",
      "!!!! visted list is 3\n",
      "len of remainng list ['3', 'aromatic ring']\n",
      "!!!! visted list is 2\n",
      "len of remainng list []\n",
      "current layer is:  0\n",
      "index : 0  To elu\u001b[31mc\u001b[0midate the roles of \u001b[31mCa\u001b[0m(2+) in this cluster and the possible location of water substrates in this process, we crystallized \u001b[\u001b[31m3\u001b[0m1mSr\u001b[0m(2+)-substituted \u001b[31mPSII\u001b[0m from Thermosynechococcus vulcanus, analyzed its crystal structure at a resolution of 2.1 Å, and compared it with the \u001b[31m1.9\u001b[0m Å structure of native PSII.\n",
      "\n",
      "index : 1  Inta\u001b[31mc\u001b[0mt \u001b[\u001b[31m3\u001b[0m1mMccA\u001b[0m tightly bound SO2 at haem 2, a dehydration product of the substrate \u001b[31msulfite\u001b[0m that was partially turned over due to photo\u001b[31mreduction\u001b[0m by X-ray irradiation, yielding the reaction intermediate SO.\n",
      "\n",
      "index : 2  Furthermore, temperature-dependent line-width broadening in partially redu\u001b[31mc\u001b[0med samples established that the \u001b[31maromatic ring\u001b[0m at position 4\u001b[31m3\u001b[0m participates in the control of the kinetics of intramolecular electron transfer.\n",
      "\n",
      "!!!! visted list is 5\n",
      "len of remainng list ['haems', 'about', 'dimer', 'porphyrin', 'Thionville']\n",
      "!!!! visted list is 2\n",
      "len of remainng list ['dimer', 'porphyrin', 'Thionville']\n",
      "!!!! visted list is 1\n",
      "len of remainng list ['porphyrin', 'Thionville']\n",
      "!!!! visted list is 1\n",
      "len of remainng list ['Thionville']\n",
      "!!!! visted list is 1\n",
      "len of remainng list []\n",
      "current layer is:  1\n",
      "index : 0  In addition, differences in subunit tertiary structure \u001b[31massociated\u001b[0m with the \u001b[31mT\u001b[0m-to-T(High) \u001b[31mtransitions\u001b[0m suggest two stereochemical pathways (one associated with the alpha subunits and one associated with the betasubunits) by which ligand binding specifically disrupts \u001b[31mquaternary\u001b[0m \u001b[31mconstraints\u001b[0m in the Trp37beta cluster.\n",
      "\n",
      "index : 1  At the alpha \u001b[31mhaems\u001b[0m, the normals to the mean pyrrole planes are tilted uniformly toward the haem centre, by \u001b[31mabout\u001b[0m three degrees relative to the haem normal, and there is a folding of about four degrees of the haem about an axis running between the methene carbons that are between the pyrrole rings bearing like-type side-chains.\n",
      "\n",
      "index : 2   sleep disturbance of elderly caregivers was \u001b[31massociated\u001b[0m with physiologic markers of cardiovascular risk, including plasma norepinephrine, epinephrine, and the hemostasis marker D-\u001b[31mdimer\u001b[0m.\n",
      "\n",
      "index : 3  A reversed-phase high-performance liquid chromatography-mass spectrometry (LC-MS) method is described for the separation and simultaneous analysis of \u001b[31mporphyrin\u001b[0ms related to disorders of heme biosynthesis (uro-, heptacarboxylic, hexacarboxylic, pentacarboxylic, and coproporphyrins).\n",
      "\n",
      "index : 4  Analysis of oxygen equilibrium data within the framework of the two-state allosteric model indicates that the structure of deoxy Hb \u001b[31mT\u001b[0mhionville is stabilized relative to that of deoxy Hb A.\n",
      "\n",
      "!!!! visted list is 5\n",
      "len of remainng list ['quinol', 'menaquinol', 'gas', 'heme pocket', 'nitric oxide']\n",
      "!!!! visted list is 2\n",
      "len of remainng list ['quinol', 'menaquinol', 'heme pocket']\n",
      "!!!! visted list is 2\n",
      "len of remainng list ['heme pocket']\n",
      "!!!! visted list is 1\n",
      "len of remainng list []\n",
      "current layer is:  2\n",
      "index : 0  A key molecular event during \u001b[31mNO\u001b[0m-dependent activation of \u001b[31mH\u001b[0m-\u001b[31mNOX\u001b[0m proteins is rupture of the heme-histidine bond and formation of a \u001b[31mfive\u001b[0m-\u001b[31mcoordinate\u001b[0m nitrosyl complex.\n",
      "\n",
      "index : 1  Cytotoxic concentrations of \u001b[31mnitric oxide\u001b[0m are generated luminally at the \u001b[31mgas\u001b[0mtroesophageal junction through the entero-salivary recirculation of dietary nitrate in humans.\n",
      "\n",
      "index : 2  In this work, the mena\u001b[31mquinol\u001b[0m oxidation site of Nrf\u001b[31mH\u001b[0m was characterized by the determination of the X-ray structure of Desulfovibrio vulgaris NrfHA nitrite reductase complex bound to 2-heptyl-4-hydroxyquinoline-N-oxide, which is shown to act as a competitive inhibitor of NrfH quinol oxidation activity.\n",
      "\n",
      "index : 3  Below about 200 K in 75% glycerol/water solvent, ligand rebinding occurs from the \u001b[31mheme pocket\u001b[0m and is nonexponential in time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.summarization.bm25 import get_bm25_weights\n",
    "from rank_bm25 import BM25Okapi\n",
    "import json\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the json file from kevin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../output_data/tmp/article_pool.json', 'r') as input_file:\n",
    "    phrase_list = json.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_layer_1 = pd.DataFrame(phrase_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the layer number\n",
    "layer_num = len(phrase_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the pair of unique phrase in layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_phrase_list = np.unique(data_df_layer_1['phrase'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ATCC Desulfovibrio desulfuricans', 'ATCC catalytic activity',\n",
       "       'ATCC partially reduced', 'ATCC photosystem II', 'ATCC reaction',\n",
       "       'Desulfovibrio desulfuricans catalytic activity',\n",
       "       'Desulfovibrio desulfuricans partially reduced',\n",
       "       'Desulfovibrio desulfuricans photosystem II',\n",
       "       'Desulfovibrio desulfuricans reaction', 'aromatic ring ATCC',\n",
       "       'aromatic ring Desulfovibrio desulfuricans',\n",
       "       'aromatic ring catalytic activity', 'aromatic ring copper',\n",
       "       'aromatic ring partially reduced', 'aromatic ring photosystem II',\n",
       "       'aromatic ring reaction', 'aromatic ring redox',\n",
       "       'catalytic activity partially reduced',\n",
       "       'catalytic activity photosystem II', 'catalytic activity reaction',\n",
       "       'copper ATCC', 'copper Desulfovibrio desulfuricans',\n",
       "       'copper catalytic activity', 'copper partially reduced',\n",
       "       'copper photosystem II', 'copper reaction', 'copper redox',\n",
       "       'partially reduced photosystem II', 'partially reduced reaction',\n",
       "       'photosystem II reaction', 'redox ATCC',\n",
       "       'redox Desulfovibrio desulfuricans', 'redox catalytic activity',\n",
       "       'redox partially reduced', 'redox photosystem II',\n",
       "       'redox reaction', 'sulfite ATCC',\n",
       "       'sulfite Desulfovibrio desulfuricans', 'sulfite aromatic ring',\n",
       "       'sulfite catalytic activity', 'sulfite copper',\n",
       "       'sulfite partially reduced', 'sulfite photosystem II',\n",
       "       'sulfite reaction', 'sulfite redox'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_phrase_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the unique phrase in layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list = list(pd.read_json('../output_data/tmp/selected_phrases.json',typ='series')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sulfite',\n",
       " 'aromatic ring',\n",
       " 'copper',\n",
       " 'redox',\n",
       " 'ATCC',\n",
       " 'Desulfovibrio desulfuricans',\n",
       " 'catalytic activity',\n",
       " 'partially reduced',\n",
       " 'photosystem II',\n",
       " 'reaction']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create list of list(unique phrase) for current pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop throught every pair of phrase\n",
    "list_of_phrase_list = []\n",
    "for i in range(len(unique_phrase_list)):\n",
    "    tmplist =[]\n",
    "    for j in range(len(p_list)):\n",
    "        if p_list[j] in unique_phrase_list[i]:\n",
    "            tmplist.append(p_list[j])\n",
    "    list_of_phrase_list.append(tmplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentece_list(unique_phrase_list):\n",
    "    list_of_phrase_list = []\n",
    "    for i in range(len(unique_phrase_list)):\n",
    "        tmplist =[]\n",
    "        for j in range(len(p_list)):\n",
    "            if p_list[j] in unique_phrase_list[i]:\n",
    "                tmplist.append(p_list[j])\n",
    "        list_of_phrase_list.append(tmplist)\n",
    "    return list_of_phrase_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attention: Every sentence in the articles from the pool is a candidate sentence for the final summary.\n",
    "# step: compute a BM25 score for every candidate sentence\n",
    "\n",
    "# to compute BM25\n",
    "# 1. find all sentence in current layer(done)\n",
    "# 2. index all sentence(done)\n",
    "# 3. compute score for all current sentence\n",
    "# 4. question here-> should we use the sentence with highest score \n",
    "# to cover the phrase? or to cover pair of phrase?\n",
    "# If so, after phrase been recoverd, tag the sentence been used,\n",
    "# recompute the BM25 in unused sentence pool\n",
    "# and iterate all sentece until all phrase been coverd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the article pool now\n",
    "article_list = data_df_layer_1['article'].values\n",
    "#for each article, find all sentence\n",
    "article_list[0]\n",
    "sentence_dic = {}\n",
    "list_sentence = []\n",
    "s_count = 0 #sentence index\n",
    "for i in range(len(article_list)):\n",
    "    #for every sentence, if not in sentence_list, push sentence in list\n",
    "    tmp_sentence_list = article_list[i].split(\".\")    \n",
    "    for j in range(len(tmp_sentence_list)):\n",
    "#         if tmp_sentence_list[j] not in article_list:\n",
    "        sentence_dic[s_count] = tmp_sentence_list[j]\n",
    "        list_sentence.append(tmp_sentence_list[j])        \n",
    "        s_count +=1\n",
    "list_sentence = np.unique(list_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_pool(data_df_layer_1):\n",
    "    article_list = data_df_layer_1['article'].values\n",
    "    #for each article, find all sentence\n",
    "    article_list[0]\n",
    "    sentence_dic = {}\n",
    "    list_sentence = []\n",
    "    s_count = 0 #sentence index\n",
    "    for i in range(len(article_list)):\n",
    "        #for every sentence, if not in sentence_list, push sentence in list\n",
    "        tmp_sentence_list = article_list[i].split(\".\")\n",
    "        for j in range(len(tmp_sentence_list)):\n",
    "#          if tmp_sentence_list[j] not in article_list:\n",
    "            sentence_dic[s_count] = tmp_sentence_list[j]\n",
    "            list_sentence.append(tmp_sentence_list[j])\n",
    "            s_count +=1\n",
    "    return np.unique(list_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25_score(sentence_list,unique_phrase_list,p_list):\n",
    "    #at each iteration\n",
    "    #find the sentence that could lead to the highest bm25\n",
    "        #1.for every pair of phrase, find the max score of that sentence, save that score and the respective sentence,\n",
    "        #2.pick the highest score, also recover that sentence\n",
    "        #3.find how many phrase this sentece been touched, tag those phrase, pop those phrase out of phrase list\n",
    "        #4.if pair of phrase that lead to highest score contains all some at least two of phrase that been poped out,\n",
    "        #5.pop out that phrase pair in pair list\n",
    "        #6.finsihed current iteration\n",
    "        #7.check whether the lenght of phrass list becomes 0 or num of iteration hit max\n",
    "        #8.if either happens, exit the problem, return the sentence list, and the touched phrase list\n",
    "        \n",
    "    \n",
    "    answer_sentence_list = []\n",
    "    touched_phrase_list = []\n",
    "    count = 0\n",
    "    while len(p_list) > 0 and len(touched_phrase_list) < len(p_list):\n",
    "        #step1\n",
    "        #create a data structure to save score for current query(as pair of phrase)\n",
    "        #in the future, implement the max-heap ds here O(nlogn) push, O(1) peek\n",
    "        score_list_current_iter = []\n",
    "        sentence_idx_list = []\n",
    "        cur_pair_phrase_dic = {}\n",
    "        for i in range(len(unique_phrase_list)):\n",
    "            #compute bm25 score use current phrase pair as query\n",
    "            bm25 = BM25Okapi(sentence_list) #create class of bm25\n",
    "            doc_scores = bm25.get_scores(unique_phrase_list[i])\n",
    "            sentence_loc = np.argmax(doc_scores)  #the index num of max-score sentence in the sen_list\n",
    "            sentence_score = np.max(doc_scores)   #the max score over all score of sentence for current query\n",
    "            score_list_current_iter.append(sentence_score)\n",
    "            sentence_idx_list.append(sentence_loc)\n",
    "            if sentence_score not in cur_pair_phrase_dic:\n",
    "                cur_pair_phrase_dic[sentence_score] = i\n",
    "        #step2\n",
    "        highest_score_index = np.argmax(score_list_current_iter)\n",
    "        high_sen = sentence_list[sentence_idx_list[highest_score_index]] \n",
    "        answer_sentence_list.append(high_sen)\n",
    "        #step three\n",
    "        cur_touched_phrase = []  #record how many phrase been touched by this sentence\n",
    "#         print('current lengh of p_list is', len(p_list))\n",
    "        list_deleted = []\n",
    "        for pos in range(len(p_list)):\n",
    "#             print('current pos is', pos)\n",
    "#             print('what is that ', p_list[pos])\n",
    "            if p_list[pos] in high_sen:\n",
    "                if p_list[pos] not in cur_touched_phrase:\n",
    "                    cur_touched_phrase.append(p_list[pos])\n",
    "#                 p_list.remove(p_list[pos])\n",
    "                #remove phrase list\n",
    "        #<wait>\n",
    "#         for i in range(len(cur_touched_phrase)):\n",
    "#             if cur_touched_phrase[i] in p_list:\n",
    "#                 p_list.remove(cur_touched_phrase[i])\n",
    "        #<wait>\n",
    "#         print('current sentence is', sentence_list[highest_score_index])\n",
    "#         print('len of unique pair  list', len(unique_phrase_list))\n",
    "        print('lengh of touched phrase', len(cur_touched_phrase))\n",
    "#         print('current count is', count)\n",
    "#         print('lengh of sentence list', len(sentence_list))\n",
    "\n",
    "#         #if the current pair of phrase that lead to this sentence which has max score contains two phrase in cur_touched_phrase\n",
    "#         #pop this pair of phrase out of pair of phrase list\n",
    "#         #return the pair of phrase by useing dic\n",
    "        curpair = unique_phrase_list[cur_pair_phrase_dic[np.max(score_list_current_iter)]] \n",
    "#         #count how many phrase in cur_touched_phrase \n",
    "        count_now = 0\n",
    "        for loc in range(len(cur_touched_phrase)):\n",
    "            if cur_touched_phrase[loc] in curpair:\n",
    "                count_now+=1\n",
    "        if count_now>=2 and curpair in unique_phrase_list:\n",
    "#             print('hahahahahahhaa six six six')\n",
    "            unique_phrase_list.remove(curpair)\n",
    "        count +=1\n",
    "        #remove phrase list\n",
    "        #unique_phrase_list.remove(curpair)\n",
    "        #remove sentence\n",
    "        sentence_list.remove(high_sen)\n",
    "        \n",
    "        #add cur touched list to total touched list\n",
    "        for inx in range(len(cur_touched_phrase)):\n",
    "            if cur_touched_phrase[inx] not in touched_phrase_list:\n",
    "                touched_phrase_list.append(cur_touched_phrase[inx])\n",
    "        print('now total touched length', len(touched_phrase_list))\n",
    "        #create a pair phrase function based on the cur_touched_phrase_process\n",
    "        tmp_phrase_pair = []\n",
    "        for p in range(len(cur_touched_phrase)):\n",
    "            for q in range(p,len(cur_touched_phrase)):\n",
    "                if cur_touched_phrase[p] != cur_touched_phrase[q]:\n",
    "                    tmp_phrase_pair.append([cur_touched_phrase[p],cur_touched_phrase[q]])\n",
    "        #iter thorught unique_phrase_list if pair is exist in touched list, dequeue them\n",
    "        \n",
    "        deletelist = []\n",
    "#         print('tmo coutched list is', len(tmp_phrase_pair))\n",
    "#         print('tmp phrase pari look like ', tmp_phrase_pair)\n",
    "        for p2 in range(len(tmp_phrase_pair)):\n",
    "            for q2 in range(len(unique_phrase_list)):\n",
    "                if tmp_phrase_pair[p2][0] in unique_phrase_list[q2] and tmp_phrase_pair[p2][1] in unique_phrase_list[q2]:\n",
    "                    if unique_phrase_list[q2] not in deletelist:\n",
    "                        deletelist.append(unique_phrase_list[q2])\n",
    "        \n",
    "#         print('look of deletelist, ', deletelist)\n",
    "#         print('len of delete list', len(deletelist))\n",
    "        for p3 in range(len(deletelist)):\n",
    "            if deletelist[p3] in unique_phrase_list:\n",
    "                unique_phrase_list.remove(deletelist[p3])\n",
    "#         print('len of unique list is', len(unique_phrase_list))\n",
    "        print('num of iteration now is', count)\n",
    "\n",
    "    return answer_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cover(sentence_list,unique_phrase_list,p_list):\n",
    "    #at each iteration\n",
    "        #find the sentence that cover most number of unvisted phrase\n",
    "        #mark those phrase as visited (pop)\n",
    "        #mark the sentence as visited (pop)\n",
    "    answer_sentence_list = []\n",
    "    touch_count_dic = []\n",
    "    count = 0\n",
    "    while len(p_list) > 0:\n",
    "        #create a data structure to save how many unvisited phrase the current sentence touched\n",
    "        touch_count_dic = {} #key as num of phrase touched, value is a list of index of sentence\n",
    "        global_max_count = 0\n",
    "        for i in range(len(sentence_list)):\n",
    "            #compute num of touched\n",
    "            tmpcount = 0\n",
    "            for j in range(len(p_list)):\n",
    "                if p_list[j] in sentence_list[i]:\n",
    "                    tmpcount+=1\n",
    "            if tmpcount > global_max_count:\n",
    "                global_max_count = tmpcount\n",
    "            # save current result in dic\n",
    "            if tmpcount in touch_count_dic:\n",
    "                #return the list\n",
    "                curlist = touch_count_dic.get(tmpcount)\n",
    "                curlist.append(i)\n",
    "            else:\n",
    "                tmplist = []\n",
    "                tmplist.append(i)\n",
    "                touch_count_dic[tmpcount] = tmplist\n",
    "        #use the global max count to return lit of index of sentence that lead to the max current count\n",
    "        \n",
    "        list_of_max_index_sentence = touch_count_dic[global_max_count]\n",
    "        #pick the first one\n",
    "        selected_max_sentence_index = list_of_max_index_sentence[0]\n",
    "        selected_max_sentence = sentence_list[selected_max_sentence_index]\n",
    "        #set cover\n",
    "        visited_list = []\n",
    "#         print('what is sentence now', selected_max_sentence)\n",
    "        \n",
    "        for loc in range(len(p_list)):\n",
    "            if p_list[loc] in selected_max_sentence and p_list[loc] not in visited_list:\n",
    "                visited_list.append(p_list[loc])\n",
    "        #delete all visited list\n",
    "        print('!!!! visted list is', len(visited_list))\n",
    "#         print('what is sentence now', selected_max_sentence)\n",
    "        for pos2 in range(len(visited_list)):\n",
    "            p_list.remove(visited_list[pos2])\n",
    "        answer_sentence_list.append(selected_max_sentence)\n",
    "        #remove the current sentencn\n",
    "        sentence_list.pop(selected_max_sentence_index)\n",
    "#         print('length of sentence list is', len(sentence_list))\n",
    "#         print('len of remainng list', p_list)\n",
    "    return answer_sentence_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotating_function(answer,p_list): #only mark the first occurance of a phrase exist in sentence\n",
    "    #iter through every answer\n",
    "    for i in range(len(answer)):\n",
    "        for j in range(len(p_list)):\n",
    "            if p_list[j] in answer[i]:\n",
    "                #find the starting index\n",
    "                start = answer[i].find(p_list[j])\n",
    "                end = start + len(p_list[j])\n",
    "                answer[i] = answer[i][0:start] + colored(p_list[j],'red') + answer[i][end:]\n",
    "    for i in range(len(answer)):\n",
    "        print('index :', i, '', answer[i] +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_func():\n",
    "    with open('../output_data/tmp/article_pool.json', 'r') as input_file:\n",
    "        phrase_list = json.load(input_file)\n",
    "    layer_num = len(phrase_list)  #how many layer\n",
    "    for i in range(layer_num):\n",
    "        data_df_layer_1 = pd.DataFrame(phrase_list[i])\n",
    "        unique_phrase_list = np.unique(data_df_layer_1['phrase'].values)\n",
    "        p_list = list(pd.read_json('../output_data/tmp/selected_phrases.json',typ='series')[i])\n",
    "#         list_of_phrase_list =  create_sentece_list(unique_phrase_list)\n",
    "        list_sentence = create_sentence_pool(data_df_layer_1)\n",
    "        #run\n",
    "        answer = set_cover(list(list_sentence),list(unique_phrase_list),p_list.copy())\n",
    "        #save it to json file\n",
    "        path = \"../output_data/out_2d_layer_\" + str(i)\n",
    "\n",
    "        with open(path, 'w') as outfile:\n",
    "            json.dump(answer, outfile)\n",
    "            \n",
    "        print('current layer is: ',i )\n",
    "        annotating_function(answer.copy(),p_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!! visted list is 3\n",
      "!!!! visted list is 2\n",
      "!!!! visted list is 2\n",
      "!!!! visted list is 2\n",
      "!!!! visted list is 1\n",
      "current layer is:  0\n",
      "index : 0  Accumulation of reduced pheophytin a (Pheo-D1) in \u001b[31mphotosystem II\u001b[0m \u001b[31mreaction\u001b[0m center (PSII RC) under illumination at low \u001b[31mredox\u001b[0m potential is accompanied by changes in absorbance and circular dichroism spectra\n",
      "\n",
      "index : 1   Furthermore, temperature-dependent line-width broadening in \u001b[31mpartially reduced\u001b[0m samples established that the \u001b[31maromatic ring\u001b[0m at position 43 participates in the control of the kinetics of intramolecular electron transfer\n",
      "\n",
      "index : 2   It is distinct from known \u001b[31msulfite\u001b[0m reductases because it has a substantially higher \u001b[31mcatalytic activity\u001b[0m and a relatively low reactivity towards nitrite\n",
      "\n",
      "index : 3   The nine-haem cytochrome c (9Hcc), previously described as having 12 haem groups, was isolated from cells of \u001b[31mDesulfovibrio desulfuricans\u001b[0m \u001b[31mATCC\u001b[0m 27774, grown under both nitrate- and sulphate-respiring conditions\n",
      "\n",
      "index : 4   A better indicator for \u001b[31mcopper\u001b[0m supply may be the evaluation of those food items, which are the predominant sources of copper intake at the population level\n",
      "\n",
      "!!!! visted list is 3\n",
      "!!!! visted list is 2\n",
      "!!!! visted list is 2\n",
      "!!!! visted list is 1\n",
      "!!!! visted list is 1\n",
      "!!!! visted list is 1\n",
      "current layer is:  1\n",
      "index : 0   This indicates that the \u001b[31msulfate\u001b[0m binding site in \u001b[31mhuman\u001b[0m hemoglobin also serves as a Cl- binding site, and that the amino-terminal Val beta 1 is essential for oxygen-linked Cl- binding to hemoglobin as well as the Cl(-)-dependent \u001b[31mBohr effect\u001b[0m\n",
      "\n",
      "index : 1   (2001) Biochemistry 40, 12357-12368] has identified the major region of \u001b[31mquaternary\u001b[0m constraint to be a cluster of residues at the \u001b[31malpha1beta2 interface\u001b[0m that is centered at Trp37beta\n",
      "\n",
      "index : 2   Analysis of oxygen equilibrium data within the framework of the two-state allosteric model indicates that the structure of \u001b[31mdeoxy Hb\u001b[0m \u001b[31mThionville\u001b[0m is stabilized relative to that of deoxy Hb A\n",
      "\n",
      "index : 3   (Gerberding \u001b[31met al\u001b[0m 1990, Panlilio et al 1991, Popejoy & Fry 1991, Quebbeman et al 1991, Tokars et al 1992, Lynch & White 1993, Stringer, Infante-Rivard & Hanley 2002)\n",
      "\n",
      "index : 4   A new synthetic approach for multifunctional \u001b[31mporphyrin\u001b[0ms was developed using alpha,beta-unsaturated acyl porphyrins as versatile building blocks with yields of 44-95%\n",
      "\n",
      "index : 5   Although both hemes of \u001b[31malpha and beta\u001b[0m subunits in metHb A take a six-coordinate (6c) high-spin structure, the 406\n",
      "\n",
      "!!!! visted list is 5\n",
      "!!!! visted list is 2\n",
      "!!!! visted list is 1\n",
      "!!!! visted list is 1\n",
      "!!!! visted list is 1\n",
      "current layer is:  2\n",
      "index : 0   \u001b[31mHeme\u001b[0m-\u001b[31mnitric oxide\u001b[0m/oxygen (H-\u001b[31mNOX\u001b[0m) binding domains are a recently discovered family of heme-based \u001b[31mgas\u001b[0m \u001b[31msensor\u001b[0m proteins that are conserved across eukaryotes and bacteria\n",
      "\n",
      "index : 1  Quinol Oxidation by \u001b[31mC-Type Cytochromes\u001b[0m: Structural Characterization of the Menaquinol \u001b[31mBinding Site\u001b[0m of Nrfha\n",
      "\n",
      "index : 2   A possible conformational change of the \u001b[31mpolar\u001b[0m head group induced by cations is discussed in the light of the present results\n",
      "\n",
      "index : 3   \u001b[31mCoarse-grained molecular dynamics simulations\u001b[0m suggest that the quinol binding site of NrfH and several other respiratory enzymes lie in the head group region of the membrane, which probably facilitates proton transfer to the periplasm\n",
      "\n",
      "index : 4  Structures of Noncoordinated \u001b[31mAromatic Residue\u001b[0m Mutants in Tetraheme Cytochrome c3 from Desulfovibrio vulgaris Miyazaki F \n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

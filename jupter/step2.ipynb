{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.summarization.bm25 import get_bm25_weights\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the json file from kevin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_layer_1 = pd.read_json('../layer_1_output.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the pair of unique phrase in layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_phrase_list = np.unique(data_df_layer_1['phrase'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the unique phrase in layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list = list(pd.read_json('./top_k_output/layer_1.json',typ='series').keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sr',\n",
       " 'ATCC',\n",
       " 'sulfite',\n",
       " 'aromatic ring',\n",
       " 'Ca',\n",
       " 'Haem',\n",
       " 'Hmc',\n",
       " 'enzymatic',\n",
       " 'member',\n",
       " 'nitrate']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create list of list(unique phrase) for current pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop throught every pair of phrase\n",
    "list_of_phrase_list = []\n",
    "for i in range(len(unique_phrase_list)):\n",
    "    tmplist =[]\n",
    "    for j in range(len(p_list)):\n",
    "        if p_list[j] in unique_phrase_list[i]:\n",
    "            tmplist.append(p_list[j])\n",
    "    list_of_phrase_list.append(tmplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ATCC Ca', 'ATCC aromatic ring', 'ATCC sulfite', 'Sr ATCC',\n",
       "       'Sr Ca', 'Sr Haem', 'Sr Hmc', 'Sr aromatic ring', 'Sr enzymatic',\n",
       "       'Sr member', 'Sr nitrate', 'Sr sulfite'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_phrase_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attention: Every sentence in the articles from the pool is a candidate sentence for the final summary.\n",
    "# step: compute a BM25 score for every candidate sentence\n",
    "\n",
    "# to compute BM25\n",
    "# 1. find all sentence in current layer(done)\n",
    "# 2. index all sentence(done)\n",
    "# 3. compute score for all current sentence\n",
    "# 4. question here-> should we use the sentence with highest score \n",
    "# to cover the phrase?\n",
    "# If so, after phrase been recoverd, tag the sentence been used,\n",
    "# recompute the BM25 in unused sentence pool\n",
    "# and iterate all sentece until all phrase been coverd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the article pool now\n",
    "article_list = data_df_layer_1['article'].values\n",
    "#for each article, find all sentence\n",
    "article_list[0]\n",
    "sentence_dic = {}\n",
    "list_sentence = []\n",
    "s_count = 0 #sentence index\n",
    "for i in range(len(article_list)):\n",
    "    #for every sentence, if not in sentence_list, push sentence in list\n",
    "    tmp_sentence_list = article_list[i].split(\".\")\n",
    "    for j in range(len(tmp_sentence_list)):\n",
    "        if tmp_sentence_list[j] not in article_list:\n",
    "            sentence_dic[s_count] = tmp_sentence_list[j]\n",
    "            list_sentence.append(tmp_sentence_list[j])\n",
    "            s_count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1892"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-49-106dc0a127c9>, line 54)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-49-106dc0a127c9>\"\u001b[0;36m, line \u001b[0;32m54\u001b[0m\n\u001b[0;31m    return 0\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def BM25_score(sentence_list,unique_phrase_list,p_list,iter_num):\n",
    "    #at each iteration\n",
    "    #find the sentence that could lead to the highest bm25\n",
    "        #1.for every pair of phrase, find the max score of that sentence, save that score and the respective sentence,\n",
    "        #2.pick the highest score, also recover that sentence\n",
    "        #3.find how many phrase this sentece been touched, tag those phrase, pop those phrase out of phrase list\n",
    "        #4.if pair of phrase that lead to highest score contains all some at least two of phrase that been poped out,\n",
    "        #5.pop out that phrase pair in pair list\n",
    "        #6.finsihed current iteration\n",
    "        #7.check whether the lenght of phrass list becomes 0 or num of iteration hit max\n",
    "        #8.if either happens, exit the problem, return the sentence list, and the touched phrase list\n",
    "        \n",
    "    \n",
    "    answer_sentence_list = []\n",
    "    touched_phrase_list = []\n",
    "    count = 0\n",
    "    while len(p_list) > 0 or count < iter_num:\n",
    "        #step1\n",
    "        #create a data structure to save score for current query(as pair of phrase)\n",
    "        #in the future, implement the max-heap ds here O(nlogn) push, O(1) peek\n",
    "        score_list_current_iter = []\n",
    "        sentence_idx_list = []\n",
    "        cur_pair_phrase_dic = {}\n",
    "        for i in range(len(unique_phrase_list)):\n",
    "            #compute bm25 score use current phrase pair as query\n",
    "            bm25 = BM25Okapi(sentence_list) #create class of bm25\n",
    "            doc_scores = bm25.get_scores(unique_phrase_list[i])\n",
    "            sentence_loc = np.argmax(doc_scores)  #the index num of max-score sentence in the sen_list\n",
    "            sentence_score = np.max(doc_scores)   #the max score over all score of sentence for current query\n",
    "            score_list_current_iter.append(sentence_score)\n",
    "            sentence_idx_list.append(sentence_loc)\n",
    "            if sentence_score not in cur_pair_phrase_dic:\n",
    "                cur_pair_phrase_dic[sentence_score] = i\n",
    "        #step2\n",
    "        highest_score_index = np.argmax(score_list_current_iter)\n",
    "        high_sen = sentence_list[sentence_idx_list[highest_score_index]] \n",
    "        answer_sentence_list.append(high_sen)\n",
    "        #step three\n",
    "        cur_touched_phrase = []  #record how many phrase been touched by this sentence\n",
    "        for pos in range(len(p_list)):\n",
    "            if p_list[pos] in high_sen:\n",
    "                cur_touched_phrase.append(p_list[pos])\n",
    "                p_list.pop(p_list[pos])\n",
    "            \n",
    "        #if the current pair of phrase that lead to this sentence which has max score contains two phrase in cur_touched_phrase\n",
    "        #pop this pair of phrase out of pair of phrase list\n",
    "        #return the pair of phrase by useing dic\n",
    "        curpair = unique_phrase_list[cur_pair_phrase_dic[highest_score_index]] \n",
    "        #count how many phrase in cur_touched_phrase \n",
    "        count_now = 0\n",
    "        for loc in range(len(cur_touched_phrase)):\n",
    "            if cur_touched_phrase[loc] in curpair:\n",
    "                count_now+=1\n",
    "        if count_now>=2 and curpair in unique_phrase_list:\n",
    "            unique_phrase_list.pop(curpair)\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cover(sentence_list,unique_phrase_list,p_list):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Hello there good man!\",\n",
    "    \"It is quite windy in London\",\n",
    "    \"How is the weather today?\"\n",
    "]\n",
    "\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "\n",
    "bm25 = BM25Okapi(list_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"windy London\"\n",
    "# tokenized_query = query.split(\" \")\n",
    "\n",
    "doc_scores = bm25.get_scores(unique_phrase_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(doc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sr Ca'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_phrase_list[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' However, the roles and relationship of CSN5 and SCARA5 in hepatocellular carcinoma (HCC) remain unclear'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentence[1646]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "corpus = [\n",
    "    \"Hello there good man!\",\n",
    "    \"It is quite windy in London\",\n",
    "    \"How is the weather today?\"\n",
    "]\n",
    "\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"London windy\"\n",
    "tokenized_query = query.split(\" \")\n",
    "\n",
    "doc_scores = bm25.get_scores(tokenized_query)\n",
    "doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

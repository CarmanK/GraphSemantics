{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn #for tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import math\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix\n",
    "eps = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_list = []\n",
    "# with open('./processed/low.txt','r') as open_f:\n",
    "#     soup = BeautifulSoup(open_f.read())\n",
    "#     for p_or_l in soup.find_all([\"phrase\"]):\n",
    "# #         print(p_or_l.get_text())\n",
    "#         text_list.append(p_or_l.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_list_t = []\n",
    "# with open('./processed/low.txt','r') as open_f:\n",
    "#     reader=csv.reader(open_f,delimiter='\\n')\n",
    "#     for row in reader:\n",
    "#         text_list_t.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# June 14\n",
    "# change the '1\\n' to for different layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = []\n",
    "# with open('./1gb_model/layer1.txt','r') as open_f:\n",
    "#     contents = open_f.read()\n",
    "#     for entry in contents.split('1\\n'):\n",
    "#         doc.append(entry)\n",
    "# doc = doc[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each doc[i], get all parsed thing, and put it in another list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_parsed_list = []\n",
    "for i in range(len(doc)):\n",
    "    tmpdoc = []\n",
    "    #do it for each doc[i]\n",
    "    soup = BeautifulSoup(doc[i])\n",
    "    for p in soup.find_all([\"phrase\"]):\n",
    "        tmpdoc.append(p.get_text())\n",
    "    #save back to mother\n",
    "    doc_parsed_list.append(tmpdoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document_in_one_layer(path,parameter):\n",
    "    splitstr = str(parameter)+'\\n'\n",
    "    doc = []\n",
    "    with open(path,'r') as open_f:\n",
    "        contents = open_f.read()\n",
    "        for entry in contents.split(splitstr):\n",
    "            doc.append(entry)\n",
    "    doc = doc[1:]\n",
    "    return doc\n",
    "\n",
    "def create_parsed_list_in_one_layer(doc):\n",
    "    doc_parsed_list = []\n",
    "    for i in range(len(doc)):\n",
    "        tmpdoc = []\n",
    "        #do it for each doc[i]\n",
    "        soup = BeautifulSoup(doc[i])\n",
    "        for p in soup.find_all([\"phrase\"]):\n",
    "            tmpdoc.append(p.get_text())\n",
    "        #save back to mother\n",
    "        doc_parsed_list.append(tmpdoc)\n",
    "    return doc_parsed_list\n",
    "\n",
    "def get_doc_parsed_list(path,parameter):\n",
    "    doc = read_document_in_one_layer(path,parameter)\n",
    "    return create_parsed_list_in_one_layer(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Currently, specify the loc of three layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = './1gb_model/layer1.txt'\n",
    "path2 = './1gb_model/layer2.txt'\n",
    "path3 = './1gb_model/layer3.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to compute tf-idf-ratio score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_set(doc_list):#1\n",
    "    wordSet = set(doc_list[0])\n",
    "    for i in range(1,len(doc_list)):\n",
    "        wordSet=wordSet.union(set(doc_list[i]))\n",
    "    return wordSet\n",
    "\n",
    "\n",
    "def create_wordDict_list(doc_list,wordSet):  #depends on how many documents we have\n",
    "    wordDict_list = []\n",
    "    for i in range(len(doc_list)):\n",
    "        wordDict_list.append(dict.fromkeys(wordSet,0))\n",
    "    return wordDict_list\n",
    "\n",
    "def createPd(wordDict_list):\n",
    "    return pd.DataFrame(wordDict_list)\n",
    "\n",
    "\n",
    "def computeTF(wordDict, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(bowCount)\n",
    "    return tfDict\n",
    "\n",
    "def computeIDF(docList):\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "        \n",
    "    return idfDict\n",
    "\n",
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf\n",
    "\n",
    "\n",
    "def populate_word_dic(wordDict_list,doc_parsed_list):\n",
    "    for i in range(len(doc_parsed_list)):\n",
    "        for word in doc_parsed_list[i]:\n",
    "            wordDict_list[i][word]+=1\n",
    "    \n",
    "def computetf(wordDict_list,doc_list):\n",
    "    tf_list = []\n",
    "    for i in range(len(doc_list)):\n",
    "        tf_list.append(computeTF(wordDict_list[i],doc_list[i]))\n",
    "    return tf_list\n",
    "\n",
    "def computeidf(wordDict_list):\n",
    "    return computeIDF(wordDict_list)\n",
    "\n",
    "def compute_tf_idf_list(tf_list,idfs):\n",
    "    tfidf_list = []\n",
    "    for i in range(len(tf_list)):\n",
    "        tfidf_list.append(computeTFIDF(tf_list[i],idfs))\n",
    "        \n",
    "    return tfidf_list\n",
    "\n",
    "\n",
    "def df_ifd_generator(doc_parsed_list_uni):\n",
    "    wordSet = union_set(doc_parsed_list_uni)\n",
    "    wordDict_list = create_wordDict_list(doc_parsed_list_uni,wordSet)\n",
    "    populate_word_dic(wordDict_list,doc_parsed_list_uni)\n",
    "    tflist = computetf(wordDict_list,doc_parsed_list_uni)\n",
    "    idflist = computeidf(wordDict_list)\n",
    "    tfidf_list = compute_tf_idf_list(tflist,idflist)\n",
    "    return pd.DataFrame(tfidf_list),pd.DataFrame(tflist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create function to compute ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First create another df only contains tf for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ratio_for_current_layer(tfidf_pd_list,tf_pd_list):\n",
    "    #step1. choose a layer as target layer, set the rest layer as layers used to compute ratio\n",
    "    #step2. loop through every phrase in target layer\n",
    "        #2.1for current phrase\n",
    "            #find the max tf of this phrase in other layer\n",
    "                #if phrase not exist in other layer, ratio = A\n",
    "                #else ratio = A/B, where B is max tf of this phrase from other layer\n",
    "        #2.2create the scoore for current phrase in current layer\n",
    "    #step3.return all result\n",
    "    for i in range(len(tfidf_pd_list)):\n",
    "        tfidflist_used_for_ratio = []\n",
    "        tflist_used_for_ratio = []\n",
    "        for j in range(len(tfidf_pd_list)):\n",
    "            if j!=i:\n",
    "                tfidflist_used_for_ratio.append(tfidf_pd_list[j])\n",
    "                tflist_used_for_ratio.append(tf_pd_list[j])\n",
    "        #other layer except for layer i has been found\n",
    "        #proceed to step2.1\n",
    "        #create a list contains all key in current layer\n",
    "        key_list = tfidf_pd_list[i].keys()\n",
    "        for loc in range(len(key_list)):\n",
    "            #if phrase not exist in both other layers\n",
    "            #create a index list that contains the index which layer has curretn loc\n",
    "            index_list_contains_current_phrase = []\n",
    "            for pos in range(len(tfidflist_used_for_ratio)):\n",
    "                if key_list[loc] in tfidflist_used_for_ratio[pos]:\n",
    "                    index_list_contains_current_phrase.append(pos)\n",
    "            #if phrase  exist in other layers:\n",
    "#             print('curretn key is ',key_list[loc], ' current index list is',index_list_contains_current_phrase )\n",
    "        \n",
    "            if len(index_list_contains_current_phrase) != 0:\n",
    "                #find the max tf of this phrase\n",
    "                tmpmax = 0\n",
    "                for exist_pos in range(len(index_list_contains_current_phrase)):\n",
    "                    cur_max = np.max(tflist_used_for_ratio[index_list_contains_current_phrase[exist_pos]][key_list[loc]].values)\n",
    "                    if cur_max > tmpmax:\n",
    "                        tmpmax = cur_max\n",
    "#                 print('what is i', i, ' what is key', key_list[loc])\n",
    "                #update current tfidf value\n",
    "                tfidf_pd_list[i][key_list[loc]] = (tfidf_pd_list[i][key_list[loc]] +eps )/(tmpmax+eps)\n",
    "            else:\n",
    "                #no document contains phrase a in current layer\n",
    "#                 print('hahah')\n",
    "                tfidf_pd_list[i][key_list[loc]] = (tfidf_pd_list[i][key_list[loc]] +eps )/(0+eps)\n",
    "    return tfidf_pd_list\n",
    "\n",
    "def generate_top_k_pd(ratio_list):\n",
    "    list_store_top_layer_record = []\n",
    "    for i in range(len(ratio_list)):\n",
    "        #find key list\n",
    "        tmp_dic = {}\n",
    "        key_list_new = ratio_list[i].keys()\n",
    "        for j in range(len(key_list_new)):\n",
    "            #find the max val for curretn key in current layer\n",
    "            tmp_dic[key_list_new[j]] =  np.max(ratio_list[i][key_list_new[j]].values)\n",
    "        list_store_top_layer_record.append(pd.Series(tmp_dic))\n",
    "    return list_store_top_layer_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate tf-idf pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_parsed_list1 = get_doc_parsed_list(path1,1)\n",
    "doc_parsed_list2 = get_doc_parsed_list(path2,2)\n",
    "doc_parsed_list3 = get_doc_parsed_list(path3,3)\n",
    "\n",
    "pd1,tflist1_pd= df_ifd_generator(doc_parsed_list1)\n",
    "pd2,tflist2_pd= df_ifd_generator(doc_parsed_list2)\n",
    "pd3,tflist3_pd= df_ifd_generator(doc_parsed_list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pd_list = []\n",
    "tfidf_pd_list.append(pd1)\n",
    "tfidf_pd_list.append(pd2)\n",
    "tfidf_pd_list.append(pd3)\n",
    "tf_pd_list = []\n",
    "tf_pd_list.append(tflist1_pd)\n",
    "tf_pd_list.append(tflist2_pd)\n",
    "tf_pd_list.append(tflist3_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list = compute_ratio_for_current_layer(tfidf_pd_list,tf_pd_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = generate_top_k_pd(ratio_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find top k for layer1\n",
    "# Save result to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sr               8.767902e+06\n",
       "ATCC             7.781514e+06\n",
       "sulfite          5.693791e+06\n",
       "aromatic ring    4.489335e+06\n",
       "Ca               4.383952e+06\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[0].nlargest(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = final[0].nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to_json('./top_k_output/layer_1.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find top k for layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bacillus                     2.697953e+07\n",
       "Crystal                      2.697953e+07\n",
       "Phosphoribosyltransferase    2.697953e+07\n",
       "Xanthine                     2.697953e+07\n",
       "haemoglobin                  8.519853e+06\n",
       "quaternary                   5.842302e+06\n",
       "Cl                           5.679902e+06\n",
       "NCS                          5.679902e+06\n",
       "methionine                   5.019449e+06\n",
       "Hb                           4.900489e+06\n",
       "dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[1].nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = final[1].nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.to_json('./top_k_output/layer_2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find top k for layer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Aromatic Residue    1.397940e+07\n",
       "Cytochrome          1.397940e+07\n",
       "Mutants             1.397940e+07\n",
       "c3                  1.397940e+07\n",
       "NOX                 5.052796e+06\n",
       "polar               2.254743e+06\n",
       "gas                 1.684266e+06\n",
       "nitric oxide        1.684266e+06\n",
       "sensor              1.684266e+06\n",
       "Binding Site        1.127372e+06\n",
       "dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[2].nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = final[2].nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.to_json('./top_k_output/layer_3.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

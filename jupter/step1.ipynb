{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn #for tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import math\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix\n",
    "eps = 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# June 14\n",
    "# change the '1\\n' to for different layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = []\n",
    "# with open('./1gb_model/layer1.txt','r') as open_f:\n",
    "#     contents = open_f.read()\n",
    "#     for entry in contents.split('1\\n'):\n",
    "#         doc.append(entry)\n",
    "# doc = doc[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each doc[i], get all parsed thing, and put it in another list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_parsed_list = []\n",
    "# for i in range(len(doc)):\n",
    "#     tmpdoc = []\n",
    "#     #do it for each doc[i]\n",
    "#     soup = BeautifulSoup(doc[i])\n",
    "#     for p in soup.find_all([\"phrase\"]):\n",
    "#         tmpdoc.append(p.get_text())\n",
    "#     #save back to mother\n",
    "#     doc_parsed_list.append(tmpdoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document_in_one_layer(path,parameter):\n",
    "    splitstr = str(parameter)+'\\n'\n",
    "    doc = []\n",
    "    with open(path,'r') as open_f:\n",
    "        contents = open_f.read()\n",
    "        for entry in contents.split(splitstr):\n",
    "            doc.append(entry)\n",
    "    doc = doc[1:]\n",
    "    return doc\n",
    "\n",
    "def create_parsed_list_in_one_layer(doc):\n",
    "    doc_parsed_list = []\n",
    "    for i in range(len(doc)):\n",
    "        tmpdoc = []\n",
    "        #do it for each doc[i]\n",
    "        soup = BeautifulSoup(doc[i])\n",
    "        for p in soup.find_all([\"phrase\"]):\n",
    "            tmpdoc.append(p.get_text())\n",
    "        #save back to mother\n",
    "        doc_parsed_list.append(tmpdoc)\n",
    "    return doc_parsed_list\n",
    "\n",
    "def get_doc_parsed_list(path,parameter):\n",
    "    doc = read_document_in_one_layer(path,parameter)\n",
    "    return create_parsed_list_in_one_layer(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Currently, specify the loc of three layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = './1gb_model/layer1.txt'\n",
    "path2 = './1gb_model/layer2.txt'\n",
    "path3 = './1gb_model/layer3.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to compute tf-idf-ratio score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_set(doc_list):#1\n",
    "    wordSet = set(doc_list[0])\n",
    "    for i in range(1,len(doc_list)):\n",
    "        wordSet=wordSet.union(set(doc_list[i]))\n",
    "    return wordSet\n",
    "\n",
    "\n",
    "def create_wordDict_list(doc_list,wordSet):  #depends on how many documents we have\n",
    "    wordDict_list = []\n",
    "    for i in range(len(doc_list)):\n",
    "        wordDict_list.append(dict.fromkeys(wordSet,0))\n",
    "    return wordDict_list\n",
    "\n",
    "def createPd(wordDict_list):\n",
    "    return pd.DataFrame(wordDict_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def computeIDF(docList):\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "        \n",
    "    return idfDict\n",
    "\n",
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf\n",
    "\n",
    "\n",
    "def populate_word_dic(wordDict_list,doc_parsed_list):\n",
    "    for i in range(len(doc_parsed_list)):\n",
    "        for word in doc_parsed_list[i]:\n",
    "            wordDict_list[i][word]+=1\n",
    "    \n",
    "def computeTF(wordDict, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(bowCount)\n",
    "    return tfDict\n",
    "\n",
    "\n",
    "def computetf(wordDict_list,doc_list):\n",
    "    tf_list = []\n",
    "    for i in range(len(doc_list)):\n",
    "        tf_list.append(computeTF(wordDict_list[i],doc_list[i]))\n",
    "    return tf_list\n",
    "\n",
    "def computeTF_hypter(wordDict, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = 0\n",
    "    for i in range(len(bow)):\n",
    "        bowCount += len(bow[i])\n",
    "    #create a dic that record total number of occurance \n",
    "    for j in range(len(wordDict)):\n",
    "        for word,count in wordDict[j].items():\n",
    "            if word not in tfDict:\n",
    "                tfDict[word] = count\n",
    "            else:\n",
    "                tfDict[word] = tfDict.get(word) + count\n",
    "                \n",
    "    for key in tfDict:\n",
    "        tfDict[key] = tfDict[key]/bowCount\n",
    "    return tfDict\n",
    "\n",
    "def computetf_hyper(wordDict_list,doc_list):\n",
    "    tf_list = []\n",
    "#     for i in range(len(doc_list)):\n",
    "#         tf_list.append(computeTF(wordDict_list,doc_list))\n",
    "    single_list = computeTF_hypter(wordDict_list,doc_list)\n",
    "    for _ in range(len(doc_list)):\n",
    "        tf_list.append(single_list)\n",
    "    return tf_list\n",
    "\n",
    "\n",
    "def computeidf(wordDict_list):\n",
    "    return computeIDF(wordDict_list)\n",
    "\n",
    "def compute_tf_idf_list(tf_list,idfs):\n",
    "    tfidf_list = []\n",
    "    for i in range(len(tf_list)):\n",
    "        tfidf_list.append(computeTFIDF(tf_list[i],idfs))\n",
    "        \n",
    "    return tfidf_list\n",
    "\n",
    "\n",
    "def df_ifd_generator(doc_parsed_list_uni):\n",
    "    wordSet = union_set(doc_parsed_list_uni)\n",
    "    wordDict_list = create_wordDict_list(doc_parsed_list_uni,wordSet)\n",
    "    populate_word_dic(wordDict_list,doc_parsed_list_uni)\n",
    "    tflist = computetf_hyper(wordDict_list,doc_parsed_list_uni)\n",
    "    idflist = computeidf(wordDict_list)\n",
    "    tfidf_list = compute_tf_idf_list(tflist,idflist)\n",
    "    return pd.DataFrame(tfidf_list),pd.DataFrame(tflist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create function to compute ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First create another df only contains tf for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ratio_for_current_layer(tfidf_pd_list,tf_pd_list):\n",
    "    #step1. choose a layer as target layer, set the rest layer as layers used to compute ratio\n",
    "    #step2. loop through every phrase in target layer\n",
    "        #2.1for current phrase\n",
    "            #find the max tf of this phrase in other layer\n",
    "                #if phrase not exist in other layer, ratio = A\n",
    "                #else ratio = A/B, where B is max tf of this phrase from other layer\n",
    "        #2.2create the scoore for current phrase in current layer\n",
    "    #step3.return all result\n",
    "    for i in range(len(tfidf_pd_list)):\n",
    "        tfidflist_used_for_ratio = []\n",
    "        tflist_used_for_ratio = []\n",
    "        for j in range(len(tfidf_pd_list)):\n",
    "            if j!=i:\n",
    "                tfidflist_used_for_ratio.append(tfidf_pd_list[j])\n",
    "                tflist_used_for_ratio.append(tf_pd_list[j])\n",
    "        #other layer except for layer i has been found\n",
    "        #proceed to step2.1\n",
    "        #create a list contains all key in current layer\n",
    "        key_list = tfidf_pd_list[i].keys()\n",
    "        for loc in range(len(key_list)):\n",
    "            #if phrase not exist in both other layers\n",
    "            #create a index list that contains the index which layer has curretn loc\n",
    "            index_list_contains_current_phrase = []\n",
    "            for pos in range(len(tfidflist_used_for_ratio)):\n",
    "                if key_list[loc] in tfidflist_used_for_ratio[pos]:\n",
    "                    index_list_contains_current_phrase.append(pos)\n",
    "            #if phrase  exist in other layers:\n",
    "#             print('curretn key is ',key_list[loc], ' current index list is',index_list_contains_current_phrase )\n",
    "        \n",
    "            if len(index_list_contains_current_phrase) != 0:\n",
    "                #find the max tf of this phrase\n",
    "                tmpmax = 0\n",
    "                for exist_pos in range(len(index_list_contains_current_phrase)):\n",
    "                    cur_max = np.max(tflist_used_for_ratio[index_list_contains_current_phrase[exist_pos]][key_list[loc]].values)\n",
    "                    if cur_max > tmpmax:\n",
    "                        tmpmax = cur_max\n",
    "#                 print('what is i', i, ' what is key', key_list[loc])\n",
    "                #update current tfidf value\n",
    "                tfidf_pd_list[i][key_list[loc]] = (tfidf_pd_list[i][key_list[loc]] +eps )/(tmpmax+eps)\n",
    "            else:\n",
    "                #no document contains phrase a in current layer\n",
    "#                 print('hahah')\n",
    "                tfidf_pd_list[i][key_list[loc]] = (tfidf_pd_list[i][key_list[loc]] +eps )/(0+eps)\n",
    "    return tfidf_pd_list\n",
    "\n",
    "def generate_top_k_pd(ratio_list):\n",
    "    list_store_top_layer_record = []\n",
    "    for i in range(len(ratio_list)):\n",
    "        #find key list\n",
    "        tmp_dic = {}\n",
    "        key_list_new = ratio_list[i].keys()\n",
    "        for j in range(len(key_list_new)):\n",
    "            #find the max val for curretn key in current layer\n",
    "            tmp_dic[key_list_new[j]] =  np.max(ratio_list[i][key_list_new[j]].values)\n",
    "        list_store_top_layer_record.append(pd.Series(tmp_dic))\n",
    "    return list_store_top_layer_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate tf-idf pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_parsed_list1 = get_doc_parsed_list(path1,1)\n",
    "doc_parsed_list2 = get_doc_parsed_list(path2,2)\n",
    "doc_parsed_list3 = get_doc_parsed_list(path3,3)\n",
    "\n",
    "pd1,tflist1_pd= df_ifd_generator(doc_parsed_list1)\n",
    "pd2,tflist2_pd= df_ifd_generator(doc_parsed_list2)\n",
    "pd3,tflist3_pd= df_ifd_generator(doc_parsed_list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pd_list = []\n",
    "tfidf_pd_list.append(pd1)\n",
    "tfidf_pd_list.append(pd2)\n",
    "tfidf_pd_list.append(pd3)\n",
    "tf_pd_list = []\n",
    "tf_pd_list.append(tflist1_pd)\n",
    "tf_pd_list.append(tflist2_pd)\n",
    "tf_pd_list.append(tflist3_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list = compute_ratio_for_current_layer(tfidf_pd_list,tf_pd_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = generate_top_k_pd(ratio_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find top k for layer1\n",
    "# Save result to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sr               1.734043e+06\n",
       "sulfite          1.300532e+06\n",
       "Ca               8.670219e+05\n",
       "Cu               6.502667e+05\n",
       "aromatic ring    6.502667e+05\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[0].nlargest(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = final[0].nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to_json('./top_k_output/layer_1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sr                1.734043e+06\n",
       "sulfite           1.300532e+06\n",
       "Ca                8.670219e+05\n",
       "Cu                6.502667e+05\n",
       "aromatic ring     6.502667e+05\n",
       "copper            6.502667e+05\n",
       "redox             5.316124e+05\n",
       "ATCC              4.335114e+05\n",
       "W3                4.335114e+05\n",
       "photosystem II    4.335114e+05\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find top k for layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quaternary        1.777243e+06\n",
       "transitions       8.625511e+05\n",
       "alpha1            6.900411e+05\n",
       "ligand binding    6.900411e+05\n",
       "porphyrin         6.900411e+05\n",
       "Hb                6.325378e+05\n",
       "interface         5.462827e+05\n",
       "High              5.175311e+05\n",
       "Noble             5.175311e+05\n",
       "et al             5.175311e+05\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[1].nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = final[1].nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.to_json('./top_k_output/layer_2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find top k for layer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOX                                              1.476698e+06\n",
       "gas                                              4.922334e+05\n",
       "nitric oxide                                     4.922334e+05\n",
       "polar                                            4.922334e+05\n",
       "sensor                                           4.922334e+05\n",
       "characterization                                 2.802404e+05\n",
       "Aromatic Residue                                 2.461172e+05\n",
       "Binding Site                                     2.461172e+05\n",
       "Coarse-grained molecular dynamics simulations    2.461172e+05\n",
       "Cytochrome                                       2.461172e+05\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[2].nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = final[2].nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.to_json('./top_k_output/layer_3.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

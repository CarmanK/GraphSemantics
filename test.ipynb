{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.summarization.bm25 import get_bm25_weights\n",
    "from rank_bm25 import BM25Okapi\n",
    "import json\n",
    "from termcolor import colored\n",
    "import re\n",
    "import itertools\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./output_data/tmp/article_pool.json', 'r') as input_file:\n",
    "    phrase_list = json.load(input_file)\n",
    "\n",
    "data_df_layer_1 = pd.DataFrame(phrase_list[0])\n",
    "layer_num = len(phrase_list)\n",
    "unique_phrase_list = np.unique(data_df_layer_1['phrase'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['band easy access', 'band mouth', 'band play', 'band rain',\n",
       "       'band upper', 'band water', 'easy access mouth',\n",
       "       'easy access play', 'easy access rain', 'easy access upper',\n",
       "       'easy access water', 'golf band', 'golf easy access',\n",
       "       'golf golfer', 'golf lightweight', 'golf mouth', 'golf play',\n",
       "       'golf rain', 'golf upper', 'golf water', 'golfer band',\n",
       "       'golfer easy access', 'golfer lightweight', 'golfer mouth',\n",
       "       'golfer play', 'golfer rain', 'golfer upper', 'golfer water',\n",
       "       'lightweight band', 'lightweight easy access', 'lightweight mouth',\n",
       "       'lightweight play', 'lightweight rain', 'lightweight upper',\n",
       "       'lightweight water', 'mouth play', 'mouth rain', 'mouth upper',\n",
       "       'mouth water', 'play rain', 'play upper', 'play water',\n",
       "       'upper rain', 'upper water', 'water rain'], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_phrase_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_list_fixed_points(layer_num):\n",
    "    ans = []\n",
    "    for i in range(layer_num):\n",
    "        p_list = list(pd.read_json('./output_data/tmp/selected_phrases.json',typ='series')[i])\n",
    "        for j in range(len(p_list)):\n",
    "            if p_list[j] not in ans:\n",
    "                ans.append(p_list[j])\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['golf'],\n",
       " ['golfer'],\n",
       " ['lightweight'],\n",
       " ['band'],\n",
       " ['easy access'],\n",
       " ['mouth'],\n",
       " ['play'],\n",
       " ['upper'],\n",
       " ['water'],\n",
       " ['rain'],\n",
       " ['cover', 'covered'],\n",
       " ['gas'],\n",
       " ['polyethylene'],\n",
       " ['central cavity'],\n",
       " ['centrally located']]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_list_fixed_points(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['golf'],\n",
       " ['golfer'],\n",
       " ['lightweight'],\n",
       " ['band'],\n",
       " ['easy access'],\n",
       " ['mouth'],\n",
       " ['play'],\n",
       " ['upper'],\n",
       " ['water'],\n",
       " ['rain']]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_list = list(pd.read_json('./output_data/tmp/selected_phrases.json',typ='series')[0])\n",
    "p_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['golf'],\n",
       " ['rain'],\n",
       " ['cover', 'covered'],\n",
       " ['gas'],\n",
       " ['play'],\n",
       " ['polyethylene'],\n",
       " ['golfer'],\n",
       " ['lightweight'],\n",
       " ['central cavity'],\n",
       " ['centrally located']]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_list = list(pd.read_json('./output_data/tmp/selected_phrases.json',typ='series')[1])\n",
    "p_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cover', 'covered']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = p_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'cover and not  ahahhaha'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0] in a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, False]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 in [c in a for c in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([c in a for c in b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_list = p_list_fixed_points(0)\n",
    "p_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'in <string>' requires string as left operand, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-0f773434b9dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtmplist\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mp_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_phrase_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mtmplist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlist_of_phrase_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmplist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'in <string>' requires string as left operand, not list"
     ]
    }
   ],
   "source": [
    "list_of_phrase_list = []\n",
    "for i in range(len(unique_phrase_list)):\n",
    "    tmplist =[]\n",
    "    for j in range(len(p_list)):\n",
    "        if p_list[j] in unique_phrase_list[i]:\n",
    "            tmplist.append(p_list[j])\n",
    "    list_of_phrase_list.append(tmplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentece_list(unique_phrase_list):\n",
    "    list_of_phrase_list = []\n",
    "    for i in range(len(unique_phrase_list)):\n",
    "        tmplist =[]\n",
    "        for j in range(len(p_list)):\n",
    "            if p_list[j] in unique_phrase_list[i]:\n",
    "                tmplist.append(p_list[j])\n",
    "        list_of_phrase_list.append(tmplist)\n",
    "    return list_of_phrase_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_pool(data_df_layer_1):\n",
    "    article_list = data_df_layer_1['article'].values\n",
    "    #for each article, find all sentence\n",
    "    article_list[0]\n",
    "    sentence_dic = {}\n",
    "    list_sentence = []\n",
    "    s_count = 0 #sentence index\n",
    "    for i in range(len(article_list)):\n",
    "        #for every sentence, if not in sentence_list, push sentence in list\n",
    "        tmp_sentence_list = re.split('(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', article_list[i])\n",
    "        for j in range(len(tmp_sentence_list)):\n",
    "#          if tmp_sentence_list[j] not in article_list:\n",
    "            sentence_dic[s_count] = tmp_sentence_list[j]\n",
    "            list_sentence.append(tmp_sentence_list[j])\n",
    "            s_count +=1\n",
    "    return np.unique(list_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#create the article pool now\n",
    "article_list = data_df_layer_1['article'].values\n",
    "#for each article, find all sentence\n",
    "sentence_dic = {}\n",
    "list_sentence = []\n",
    "s_count = 0 #sentence index\n",
    "for i in range(len(article_list)):\n",
    "    #for every sentence, if not in sentence_list, push sentence in list\n",
    "    tmp_sentence_list = re.split('(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', article_list[i])\n",
    "    for j in range(len(tmp_sentence_list)):\n",
    "#         if tmp_sentence_list[j] not in article_list:\n",
    "        sentence_dic[s_count] = tmp_sentence_list[j]\n",
    "        list_sentence.append(tmp_sentence_list[j])        \n",
    "        s_count +=1\n",
    "list_sentence = np.unique(list_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ven_diagram(list_sentence,unique_phrase_list,p_list):\n",
    "    #generate the list of list\n",
    "    list_of_list_com = []\n",
    "    counter = len(p_list)\n",
    "    while counter > 0:\n",
    "        list_of_list_com.append(list(itertools.combinations(p_list,counter)))\n",
    "        counter-=1\n",
    "        \n",
    "    #venn cover\n",
    "    len_p_list = len(p_list)#len of p_list\n",
    "    #find the sentence that matches current cover\n",
    "    ans = []\n",
    "    for i in range(len(list_of_list_com)):\n",
    "        for j in range(len(list_of_list_com[i])):\n",
    "            #find the phrase list that not in com_list\n",
    "            cur_list = list(list_of_list_com[i][j])\n",
    "            not_in_list = []\n",
    "            for pos in range(len(p_list)):\n",
    "                if p_list[pos] not in cur_list:\n",
    "                    not_in_list.append(p_list[pos])\n",
    "            \n",
    "            #find the sentence that cover all phrase in list_of_list_com[i][j] and not covered in not_in_list\n",
    "            for p in  range(len(list_sentence)):\n",
    "                flag_good = True\n",
    "                flag_good_filter = True\n",
    "                for q in range(len(cur_list)):\n",
    "                    if cur_list[q] not in list_sentence[p]:\n",
    "                        flag_good = False\n",
    "                        break\n",
    "                for g in range(len(not_in_list)):\n",
    "                    if not_in_list[g] in list_sentence[p]:\n",
    "                        flag_good_filter = False\n",
    "                        break\n",
    "                #pass all exam\n",
    "                #push current setence to result\n",
    "                if flag_good==True and flag_good_filter==True and list_sentence[p] not in ans:\n",
    "                    ans.append(list_sentence[p])\n",
    "                    \n",
    "        print('combination:',len_p_list,' choose ',len_p_list - i,' current lengh of ans is', len(ans))\n",
    "                    \n",
    "    return ans\n",
    "\n",
    "# a = ven_diagram(list_sentence.copy(),unique_phrase_list,p_list)\n",
    "# len(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_func():\n",
    "    with open('./output_data/tmp/article_pool.json', 'r') as input_file:\n",
    "        phrase_list = json.load(input_file)\n",
    "    layer_num = len(phrase_list)  #how many layer\n",
    "    full_output = []\n",
    "    #<change on 07032019>\n",
    "    list_phrase = []\n",
    "    allsentence = []\n",
    "    #<change on 07032019>\n",
    "    for i in range(layer_num):\n",
    "        data_df_layer_1 = pd.DataFrame(phrase_list[i])\n",
    "        unique_phrase_list = np.unique(data_df_layer_1['phrase'].values)\n",
    "        #p_list = list(pd.read_json('./output_data/tmp/selected_phrases.json',typ='series')[i])\n",
    "#         list_of_phrase_list =  create_sentece_list(unique_phrase_list)\n",
    "        p_list = p_list_fixed_points(layer_num)\n",
    "        #<change on 07032019 >\n",
    "        list_phrase.append(list(pd.read_json('./output_data/tmp/selected_phrases.json',typ='series')[i]))\n",
    "        #<change on 07032019 >\n",
    "        list_sentence = create_sentence_pool(data_df_layer_1)\n",
    "        #run\n",
    "        answer = set_cover(list(list_sentence),list(unique_phrase_list),p_list.copy())\n",
    "        full_output.append(answer)\n",
    "        #change 07032019\n",
    "        allsentence += answer\n",
    "        #change 07032019\n",
    "        print('current layer is: ',i )\n",
    "        #annotating_function(answer.copy(),p_list)\n",
    "        print('length of current p_list ', len(p_list))\n",
    "        \n",
    "    #<------change on 0703/2019------------->\n",
    "    #filter the selected sentence\n",
    "    \n",
    "    \n",
    "    \n",
    "    #<------change on 0703/2019------------->\n",
    "    with open('./output_data/summaries.json', 'w') as outfile:\n",
    "            json.dump(full_output, outfile, indent = 4)\n",
    "\n",
    "    a =  filter_sentence(allsentence,list_phrase)\n",
    "    new_annot(a,list_phrase)\n",
    "#%%\n",
    "# Runs the entire program\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def set_cover(sentence_list,unique_phrase_list,p_list):\n",
    "    #at each iteration\n",
    "        #find the sentence that cover most number of unvisted phrase\n",
    "        #mark those phrase as visited (pop)\n",
    "        #mark the sentence as visited (pop)\n",
    "    answer_sentence_list = []\n",
    "    touch_count_dic = []\n",
    "    count = 0\n",
    "    isempty = False\n",
    "    while len(p_list) > 0 and isempty==False:\n",
    "        #create a data structure to save how many unvisited phrase the current sentence touched\n",
    "        touch_count_dic = {} #key as num of phrase touched, value is a list of index of sentence\n",
    "        global_max_count = 0\n",
    "        for i in range(len(sentence_list)):\n",
    "            #compute num of touched\n",
    "            tmpcount = 0\n",
    "            for j in range(len(p_list)):\n",
    "                if 1 in [c in sentence_list[i] for c in p_list[j]]:\n",
    "#                 if p_list[j] in sentence_list[i]:\n",
    "                    tmpcount+=1\n",
    "    \n",
    "            if tmpcount > global_max_count:\n",
    "                global_max_count = tmpcount\n",
    "            # save current result in dic\n",
    "            if tmpcount in touch_count_dic:\n",
    "                #return the list\n",
    "                curlist = touch_count_dic.get(tmpcount)\n",
    "                curlist.append(i)\n",
    "            else:\n",
    "                tmplist = []\n",
    "                tmplist.append(i)\n",
    "                touch_count_dic[tmpcount] = tmplist\n",
    "        #use the global max count to return lit of index of sentence that lead to the max current count\n",
    "        \n",
    "        list_of_max_index_sentence = touch_count_dic[global_max_count]\n",
    "        #pick the first one\n",
    "        selected_max_sentence_index = list_of_max_index_sentence[0]\n",
    "        selected_max_sentence = sentence_list[selected_max_sentence_index]\n",
    "        #set cover\n",
    "        visited_list = []\n",
    "#         print('what is sentence now', selected_max_sentence)\n",
    "        \n",
    "        for loc in range(len(p_list)):\n",
    "            if 1 in [c in selected_max_sentence for c in p_list[loc]] and p_list[loc] not in visited_list:\n",
    "#             if p_list[loc] in selected_max_sentence and p_list[loc] not in visited_list:\n",
    "                visited_list.append(p_list[loc])\n",
    "                \n",
    "        #delete all visited list\n",
    "        print('!!!! visted list is', len(visited_list))\n",
    "        \n",
    "        \n",
    "#         print('what is sentence now', selected_max_sentence)\n",
    "        if len(visited_list) == 0:\n",
    "            isempty=True\n",
    "            break\n",
    "        for pos2 in range(len(visited_list)):\n",
    "            p_list.remove(visited_list[pos2])\n",
    "        answer_sentence_list.append(selected_max_sentence)\n",
    "        #remove the current sentencn\n",
    "        sentence_list.pop(selected_max_sentence_index)\n",
    "#         print('length of sentence list is', len(sentence_list))\n",
    "#         print('len of remainng list', p_list)\n",
    "    return answer_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "def annotating_function(answer,p_list): #only mark the first occurance of a phrase exist in sentence\n",
    "    #iter through every answer\n",
    "    for i in range(len(answer)):\n",
    "        for j in range(len(p_list)):\n",
    "            if p_list[j] in answer[i]:\n",
    "                #find the starting index\n",
    "                start = answer[i].find(p_list[j])\n",
    "                end = start + len(p_list[j])\n",
    "                answer[i] = answer[i][0:start] + colored(p_list[j],'red') + answer[i][end:]\n",
    "    for i in range(len(answer)):\n",
    "        print('index :', i, '', answer[i] +'\\n')\n",
    "\n",
    "\n",
    "def new_annot(answer,p_list):\n",
    "    for i in range(len(answer)):\n",
    "        for j in range(len(p_list[0])):\n",
    "            #first check if that exist\n",
    "            if 1 in [c in answer[i] for c in p_list[0][j]]:\n",
    "                #find the index \n",
    "                index = np.argmax([c in answer[i] for c in p_list[0][j]])\n",
    "#             if p_list[0][j] in answer[i]:\n",
    "                #find the starting index\n",
    "                start = answer[i].find(p_list[0][j][index])\n",
    "                end = start + len(p_list[0][j][index])\n",
    "                answer[i] = answer[i][0:start] + colored(p_list[0][j][index],'red') + answer[i][end:]\n",
    "                \n",
    "        for j in range(len(p_list[1])):\n",
    "            if 1 in [c in answer[i] for c in p_list[1][j]]:\n",
    "                #find the index \n",
    "                index = np.argmax([c in answer[i] for c in p_list[1][j]])\n",
    "#             if p_list[0][j] in answer[i]:\n",
    "                #find the starting index\n",
    "                start = answer[i].find(p_list[1][j][index])\n",
    "                end = start + len(p_list[1][j][index])\n",
    "                answer[i] = answer[i][0:start] + colored(p_list[1][j][index],'green') + answer[i][end:]\n",
    "                \n",
    "                \n",
    "        if len(p_list) >=3:       \n",
    "            for j in range(len(p_list[2])):\n",
    "                if 1 in [c in answer[i] for c in p_list[2][j]]:\n",
    "                    #find the index \n",
    "                    index = np.argmax([c in answer[i] for c in p_list[2][j]])\n",
    "    #             if p_list[0][j] in answer[i]:\n",
    "                    #find the starting index\n",
    "                    start = answer[i].find(p_list[2][j][index])\n",
    "                    end = start + len(p_list[2][j][index])\n",
    "                    answer[i] = answer[i][0:start] + colored(p_list[2][j][index],'blue') + answer[i][end:]\n",
    "\n",
    "                \n",
    "    for i in range(len(answer)):\n",
    "        print('index :', i, '', answer[i] +'\\n')\n",
    "        \n",
    "\n",
    "#scoring system\n",
    " # of phrases covered\n",
    " # of pairs of phrases covered\n",
    " # of layers covered\n",
    "\n",
    "def filter_sentence(full_output,list_phrase):\n",
    "    final_ans = []\n",
    "    #save sentence that beencoved from multip layer phrase and sentence that covered by at least 3 phrase in \n",
    "    for i in range(len(full_output)):\n",
    "        #iter through every sentence\n",
    "        flag_list = [0]*len(list_phrase)\n",
    "        for j in range(len(list_phrase)):\n",
    "            #at current \n",
    "            for pos in range(len(list_phrase[j])):\n",
    "                if 1 in [c in full_output[i] for c in list_phrase[j][pos]]:\n",
    "                #if list_phrase[j][pos] in full_output[i]:\n",
    "                    flag_list[j] = 1\n",
    "                    break\n",
    "        if flag_list.count(1) >=2:\n",
    "            \n",
    "            if full_output[i] not in final_ans:\n",
    "                final_ans.append(full_output[i])\n",
    "        \n",
    "        count_list =[0]*len(list_phrase)\n",
    "        for j2 in range(len(list_phrase)):\n",
    "            counter = 0\n",
    "            for pos2 in range(len(list_phrase[j])):\n",
    "                if 1 in [c in full_output[i] for c in list_phrase[j2][pos2]]:\n",
    "                #if list_phrase[j2][pos2] in full_output[i]:\n",
    "                    counter+=1\n",
    "            count_list[j2] = counter\n",
    "        \n",
    "#         print('what is count_list', count_list)\n",
    "        if np.max(count_list) >=3:\n",
    "            if full_output[i] not in final_ans:\n",
    "                final_ans.append(full_output[i])\n",
    "            \n",
    "    return final_ans\n",
    "#%%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!! visted list is 4\n",
      "!!!! visted list is 3\n",
      "!!!! visted list is 2\n",
      "!!!! visted list is 1\n",
      "!!!! visted list is 1\n",
      "!!!! visted list is 1\n",
      "!!!! visted list is 1\n",
      "!!!! visted list is 0\n",
      "current layer is:  0\n",
      "length of current p_list  15\n",
      "!!!! visted list is 4\n",
      "!!!! visted list is 3\n",
      "!!!! visted list is 2\n",
      "!!!! visted list is 2\n",
      "!!!! visted list is 1\n",
      "!!!! visted list is 1\n",
      "!!!! visted list is 1\n",
      "!!!! visted list is 1\n",
      "current layer is:  1\n",
      "length of current p_list  15\n",
      "index : 0  A \u001b[31mband\u001b[0m of elastic surrounds and gathers the base and the top portions thereof in pleats, and the sleeve is sufficiently elongated so that when the base is fitted over the \u001b[31mmouth\u001b[0m of the bag, the \u001b[31mupper\u001b[0m portion of the \u001b[32mcover\u001b[0m will fold over the club heads.\n",
      "\n",
      "index : 1  A flexible, \u001b[31mwater\u001b[0mproof, \u001b[31m\u001b[32mlightweight\u001b[0m\u001b[0m compact \u001b[32mcover\u001b[0m for the top of a \u001b[31m\u001b[32mgolf\u001b[0m\u001b[0m bag and for golf clubs having a first end and a second end.\n",
      "\n",
      "index : 2  A 7-week, 10-session individual t\u001b[31m\u001b[32mrain\u001b[0m\u001b[0ming program was implemented with a youth elite football (soccer) \u001b[31m\u001b[32mplay\u001b[0m\u001b[0mer who had been underperforming because of poor aerobic fitness.\n",
      "\n",
      "index : 3  At the end of the period, the \u001b[31m\u001b[32mgolf\u001b[0m\u001b[0mer was able to \u001b[31m\u001b[32mplay\u001b[0m\u001b[0m and practice without LBP.\n",
      "\n",
      "index : 4  Graft-versus-host disease in the \u001b[31mupper\u001b[0m \u001b[32mgas\u001b[0mtrointestinal tract presents with anorexia, vomiting, and abdominal discomfort.\n",
      "\n",
      "index : 5  The \u001b[32mcover\u001b[0m (10) is composed of a clear, flexible, \u001b[31mwater\u001b[0m impervious plastic, such as \u001b[32mpolyethylene\u001b[0m.\n",
      "\n",
      "index : 6  The \u001b[32mcover\u001b[0m of this invention keeps clubs dry while in the bag, and, in addition, provides \u001b[31measy access\u001b[0m and an unobstructed view thereof to facilitate selecting a club for \u001b[31m\u001b[32mplay\u001b[0m\u001b[0m during such weather.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main_func_new():\n",
    "    with open('./output_data/tmp/article_pool.json', 'r') as input_file:\n",
    "        phrase_list = json.load(input_file)\n",
    "    layer_num = len(phrase_list)  #how many layer\n",
    "    full_output = []\n",
    "    #<change on 07032019>\n",
    "    list_phrase = []\n",
    "    allsentence = []\n",
    "    #<change on 07032019>\n",
    "    for i in range(layer_num):\n",
    "        data_df_layer_1 = pd.DataFrame(phrase_list[i])\n",
    "        unique_phrase_list = np.unique(data_df_layer_1['phrase'].values)\n",
    "        #p_list = list(pd.read_json('./output_data/tmp/selected_phrases.json',typ='series')[i])\n",
    "#         list_of_phrase_list =  create_sentece_list(unique_phrase_list)\n",
    "        p_list = p_list_fixed_points(layer_num)\n",
    "        #<change on 07032019 >\n",
    "        list_phrase.append(list(pd.read_json('./output_data/tmp/selected_phrases.json',typ='series')[i]))\n",
    "        #<change on 07032019 >\n",
    "        list_sentence = create_sentence_pool(data_df_layer_1)\n",
    "        #run\n",
    "        \n",
    "    \n",
    "        answer = set_cover(list(list_sentence),list(unique_phrase_list),p_list.copy())\n",
    "        full_output.append(answer)\n",
    "        #change 07032019\n",
    "        allsentence += answer\n",
    "        #change 07032019\n",
    "        print('current layer is: ',i )\n",
    "        #annotating_function(answer.copy(),p_list)\n",
    "        print('length of current p_list ', len(p_list))\n",
    "        \n",
    "    #<------change on 0703/2019------------->\n",
    "    #filter the selected sentence\n",
    "    \n",
    "    \n",
    "   \n",
    "    #<------change on 0703/2019------------->\n",
    "#     with open('./output_data/summaries.json', 'w') as outfile:\n",
    "#             json.dump(full_output, outfile, indent = 4)\n",
    "    \n",
    "    a =  filter_sentence(allsentence,list_phrase)\n",
    "\n",
    "    new_annot(a,list_phrase)\n",
    "    with open('./output_data/summaries.json', 'w') as outfile:\n",
    "        json.dump(a, outfile, indent = 4)\n",
    "\n",
    "#%%\n",
    "# Runs the entire program\n",
    "a = main_func_new()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
